{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# 'The Twitter Stream Grab' download and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## Updating ['dozent'](https://github.com/Social-Media-Public-Analysis/dozent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9541-09da-4983-adc8-3d7bda75e71e",
   "metadata": {},
   "source": [
    "Dozent is a powerful downloader that is used to collect large amounts of Twitter data from the Internet Archive.\n",
    "\n",
    "It had been maintained until 30th June, 2020, but the 'Twitter Stream Grab' on the Internet Archive contains data up to 30th January, 2023.\n",
    "\n",
    "There is a file in Dozent's code named 'twitter-archive-stream-links.json' which listed the links to the dataset's archives and which had to be updated to support downloads up to 30th January, 2023.\n",
    "\n",
    "A list of links, considering the content on the Internet Archive up to 30th January, 2023, was manually compiled and then processed to obtain an updated version of 'twitter-archive-stream-links.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ea9f6-03fa-444d-882b-1ed6f56183f0",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db29bd-a60d-4a8d-ac1a-e745d3a101d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e99e9-d550-4134-b511-f2a52cf1e734",
   "metadata": {},
   "source": [
    "### Determining the patterns of the links\n",
    "- Processing 'links.txt' to obtain 'twitter-archive-stream-links.json' requires that 'day', 'month' and 'year' are retrieved from each link. Thus, it is required to determine the patterns of the links\n",
    "- 'links.txt' was sorted and the resulting 'sorted_links.txt' was inspected with VS Code to identify the patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbe793-5faa-4600-a0f5-96169713794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_links = pd.read_table('links.txt', sep='\\\\n', header=None, engine='python')\n",
    "df_original_links = df_original_links.replace(' \\(View Contents\\).*$', '', regex=True).replace('\t08-Jun-2014 16:46\t5.2G.*$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c00596-4c2e-457d-8b5d-8be59805fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_links.to_csv('links.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a791049-7f9a-445b-8f8e-8c130cdf9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_links = df_original_links.sort_values(0, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ffa0c-0559-4248-a085-93dd23dad8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_links.to_csv('sorted_links.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079855f-ba4a-4ab2-93c9-b2a7f9047431",
   "metadata": {},
   "source": [
    "### Formatting 'twitter-archive-stream-links.json' from 'links.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbaf32a-c145-41c6-bbb6-fa11b5347c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_links(input_file):\n",
    "    df = pd.DataFrame(columns=['day', 'month', 'year', 'link'])\n",
    "    df['link'] = pd.read_table(input_file, sep='\\\\n', header=None, engine='python')\n",
    "    df['link'] = df['link'].replace(' \\(View Contents\\).*$', '', regex=True).replace('\t08-Jun-2014 16:46\t5.2G.*$', '', regex=True)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['link'].startswith('https://archive.org/download/archiveta'):\n",
    "            year = row['link'][80:84]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][86:88]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-json-twitterstream-2012/twitter-stream-2012-01-'):\n",
    "            year = row['link'][80:84]\n",
    "            month = row['link'][85:87]\n",
    "            day = row['link'][88:90]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-json-twitterstream/twitter-stream-2011-'):\n",
    "            year = row['link'][75:79]\n",
    "            month = row['link'][80:82]\n",
    "            day = row['link'][83:85]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-json-2011/twitter-json-scrape-2011-'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2012-'):\n",
    "            year = row['link'][84:88]\n",
    "            month = row['link'][89:91]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2013-'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2014-'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2015-'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2016-'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-01'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-02'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-03'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-04'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-05'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-06'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year    \n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-07'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-08'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-09'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-10'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2017-11'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-01'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-02'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-03'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-04'):\n",
    "            year = row['link'][91:95]\n",
    "            month = row['link'][96:98]\n",
    "            day = 'NaN'\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-05'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-06'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-07'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year    \n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-08'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-09'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-10/twitter-2018'):\n",
    "            year = row['link'][72:76]\n",
    "            month = row['link'][77:79]\n",
    "            day = row['link'][80:82]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-10/twitter_'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-11'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2018-12'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2019'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2020'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year    \n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-01'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-02'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-03'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-04'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-05'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-06'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-07'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-08/twitter-stream-2021-'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][84:86]\n",
    "            day = row['link'][87:89]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year    \n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-08/twitter-stream-20210'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-09'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-10'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-11'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2021-12'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2022'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "        elif row['link'].startswith('https://archive.org/download/archiveteam-twitter-stream-2023'):\n",
    "            year = row['link'][79:83]\n",
    "            month = row['link'][83:85]\n",
    "            day = row['link'][85:87]\n",
    "            df.at[index, 'day'] = day\n",
    "            df.at[index, 'month'] = month\n",
    "            df.at[index, 'year'] = year\n",
    "    \n",
    "    return df\n",
    "\n",
    "input_file = 'links.txt'\n",
    "df_links = format_links(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9aa6a2-223d-44a1-9ca1-5078b5936ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab54bda-df46-4626-a3a0-f49c2738f8c0",
   "metadata": {},
   "source": [
    "### Generating the formatted 'json' file for 'dozent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d69b3-d25d-40f9-83c0-1a098a33ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'twitter-archive-stream-links-updated.json'\n",
    "df_links.to_json(output_file, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9126ae-8309-4336-aadf-2e2e5631950e",
   "metadata": {},
   "source": [
    "### Formatting the resulting 'json' file\n",
    "- The resulting 'twitter-archive-stream-links-updated.json' file must be formatted with [JSON formatter](https://jsonformatter.org) considering a two-space tab and renamed to 'twitter-archive-stream-links.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f1cef-b4f5-4387-9ae7-73f381ad5e95",
   "metadata": {},
   "source": [
    "### Debugging 'twitter-archive-stream-links.json'\n",
    "- Malformed dates were detected in the following lines and manually corrected as follows:\n",
    "  - Line 1575: from '31/09/2017' to '30/09/2017'\n",
    "  - Line 1947: from '31/11/2017' to '30/11/2017'\n",
    "  - Line 5985: from '31/04/2020' to '30/04/2020'\n",
    "  - Line 6357: from '31/06/2020' to '30/06/2020'\n",
    "\n",
    "Note: Those malformed dates were causing the programme to break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6983c63-ff6d-4242-9a29-43c6635e02b0",
   "metadata": {},
   "source": [
    "## Running 'dozent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2121c2-9df1-428a-80dd-620765f8dea9",
   "metadata": {},
   "source": [
    "### Installing the 'dozent' adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d1e69-c998-4794-b40e-a9dc8413e035",
   "metadata": {},
   "source": [
    "#### The 'dozent' adaptation consists of two parts:\n",
    "- An adapted 'dozent.py' programme\n",
    "- An updated 'twitter-archive-stream-links.json' obtained from the previous section of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bd833-1fd7-4ad5-9cb7-fbc5e52e699e",
   "metadata": {},
   "source": [
    "##### Adapted 'dozent.py' programme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca20c1a-bb28-468c-8da3-c2cceed7b4dc",
   "metadata": {},
   "source": [
    "###### From"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c51395-aab6-44c2-bc04-c2fa4ba8f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIRST_DAY_OF_SUPPORT = datetime.date(2017, 6, 1)\n",
    "LAST_DAY_OF_SUPPORT = datetime.date(2020, 6, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996729d-ca3a-45c8-8f89-6e9e60fc3615",
   "metadata": {},
   "source": [
    "###### To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0876f-89d0-4edd-a0c3-c6a316b23e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIRST_DAY_OF_SUPPORT = datetime.date(2011, 9, 27)\n",
    "LAST_DAY_OF_SUPPORT = datetime.date(2023, 1, 31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9f035-3d72-470e-9815-845d46638a9c",
   "metadata": {},
   "source": [
    "##### Updated 'twitter-archive-streams-links.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcfaab-1f39-4315-ab17-4269299a2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"day\": \"27\",\n",
    "    \"month\": \"09\",\n",
    "    \"year\": \"2011\",\n",
    "    \"link\": \"https://archive.org/download/archiveteam-json-twitterstream/twitter-stream-2011-09-27.zip\"\n",
    "  },\n",
    "<omitted>\n",
    "  {\n",
    "    \"day\": \"30\",\n",
    "    \"month\": \"01\",\n",
    "    \"year\": \"2023\",\n",
    "    \"link\": \"https://archive.org/download/archiveteam-twitter-stream-2023-01/twitter-stream-20230130.tar\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7baca-128b-4a34-8b41-07c77323175a",
   "metadata": {},
   "source": [
    "The adapted 'dozent' is going to be run as part of a programme named 'tw_aws_dozent.py'\n",
    " - The code of 'tw_aws_dozent.py' is indicated as follows\n",
    " - It should be placed in '/home/ubuntu/{env}/' directory, where {env} corresponds to the virtual environment where 'dozent' is deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a53c92-c1d6-453b-8ad9-fcff1ee57a13",
   "metadata": {},
   "source": [
    "#### Upload the programme files to an AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1b454-da0b-4b1e-a388-15adfaff2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://laelgelctweets/dozent/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0aa71-4441-444b-bbe0-3ed6c06e9502",
   "metadata": {},
   "source": [
    "#### Installing the 'dozent' solution adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d3e69-a77d-40ce-8f7c-85a170799bad",
   "metadata": {},
   "source": [
    "Proceed as indicated in the document [CL_Tw_AWS_dozent_prerequisites.ipynb](https://github.com/laelgelc/tw_aws_dozent/blob/main/CL_Tw_AWS_dozent_prerequisites.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602d6cf-99ab-4942-8835-ad717df630e6",
   "metadata": {},
   "source": [
    "### There are two possibilities of execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17a1fd-f5c5-4dbe-9c18-4967db1f175a",
   "metadata": {},
   "source": [
    "#### On the terminal\n",
    "The terminal should remain open all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79571795-ccc7-4693-9aeb-a593266dcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-16-95:~/my_env$ python tw_aws_dozent.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5a1e0-03d3-45f1-8b57-b80126c78625",
   "metadata": {},
   "source": [
    "#### In the background\n",
    "The programme will be executed in the background even if the terminal session is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f85e59-5de9-421b-9088-9d8cc184bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-16-95:~/my_env$ nohup python tw_aws_dozent.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0b9cd-0950-4f4c-8e06-a4e8cf91a129",
   "metadata": {},
   "source": [
    "### Code of 'tw_aws_dozent.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb967a-635b-42ed-988b-e9194125585e",
   "metadata": {},
   "source": [
    "- 'tw_aws_dozent.py' requires 'pandas'. This prerequisite should be met in the deployment of the virtual environment\n",
    "- 'date_list' should contain the filename of the required batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930cd1f-a08d-4c74-9820-089924f79683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "date_list = 'tw_aws_dozent_date_list_test.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2011.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2012.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2013.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2014.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2015.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2016.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2017.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2018.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2019.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2020.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2021.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2022.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_2023.csv'\n",
    "#date_list = 'tw_aws_dozent_date_list_test.csv'\n",
    "\n",
    "env = 'my_env'\n",
    "bucket = 'laelgelctweets'\n",
    "\n",
    "origin = '/home/ubuntu/{}/lib/python3.10/site-packages/data/*'.format(env)\n",
    "destination = 's3://{}/'.format(bucket)\n",
    "logfile = 'tw_aws_dozent.log'\n",
    "df = pd.read_csv(date_list, header = 0)\n",
    "\n",
    "with open(logfile, 'a', encoding='utf8') as log:\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['Dates']\n",
    "        print('Processing ' + date)\n",
    "        log.write('Processing ' + date + '\\n')\n",
    "        subprocess.run(['python', '-m', 'dozent', '-s', date, '-e', date], bufsize=0)\n",
    "        files_to_copy = sorted(glob.glob(origin))\n",
    "        for file in files_to_copy:\n",
    "            subprocess.run(['aws', 's3', 'cp', file, destination], bufsize=0)\n",
    "            subprocess.run(['rm', '-f', file], bufsize=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a9fbe-8c94-4fc2-bea1-48317f239ba4",
   "metadata": {},
   "source": [
    "## Alternative solution via 'pySmartDL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8058a8-a460-49bd-9cf5-dc64669ee64c",
   "metadata": {},
   "source": [
    "- 'pySmartDL' is the heart of 'dozent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1494a-e836-4c0d-9ad1-febbc6441ff2",
   "metadata": {},
   "source": [
    "### Installing 'pandas' (in case it is not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb02098-f33c-48dc-89c0-39673c67b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-41-6:~/my_env$ pip install pandas\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd63f05-7c99-4286-91d8-8ee1e083f33d",
   "metadata": {},
   "source": [
    "### Installing 'pySmartDL' (in case it is not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef07450-e5c2-4fd8-8677-5bb9817e527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-41-6:~/my_env$ pip install pySmartDL\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7acb9d-5c7e-47b4-a68f-53534765373e",
   "metadata": {},
   "source": [
    "### Upload the programme files to an AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b30a9-8fb2-4be1-9103-a3145a374984",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://laelgelctweets/dozent/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1de78b-494d-41d8-a912-d0a83c0278fa",
   "metadata": {},
   "source": [
    "### Installing the 'tw_aws.py' solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09f847-6787-49dd-a8b1-59cccfb5aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-38-165:~$ cd my_env\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env$ ls -la\n",
    "total 24\n",
    "drwxrwxr-x 5 ubuntu ubuntu 4096 Nov  4 00:06 .\n",
    "drwxr-x--- 5 ubuntu ubuntu 4096 Nov  4 00:06 ..\n",
    "drwxrwxr-x 2 ubuntu ubuntu 4096 Nov  4 00:14 bin\n",
    "drwxrwxr-x 2 ubuntu ubuntu 4096 Nov  4 00:06 include\n",
    "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov  4 00:06 lib\n",
    "lrwxrwxrwx 1 ubuntu ubuntu    3 Nov  4 00:06 lib64 -> lib\n",
    "-rw-rw-r-- 1 ubuntu ubuntu   71 Nov  4 00:06 pyvenv.cfg\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env$ mkdir tw_aws\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env$ cd tw*\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws.py .\n",
    "download: s3://laelgelctweets/dozent/tw_aws.py to ./tw_aws.py\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_test .\n",
    "fatal error: An error occurred (404) when calling the HeadObject operation: Key \"dozent/tw_aws_links_test\" does not exist\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_test.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_test.csv to ./tw_aws_links_list_test.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2011.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2011.csv to ./tw_aws_links_list_2011.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2012.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2012.csv to ./tw_aws_links_list_2012.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2013.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2013.csv to ./tw_aws_links_list_2013.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2014.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2014.csv to ./tw_aws_links_list_2014.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2015.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2015.csv to ./tw_aws_links_list_2015.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2016.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2016.csv to ./tw_aws_links_list_2016.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2017.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2017.csv to ./tw_aws_links_list_2017.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2018.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2018.csv to ./tw_aws_links_list_2018.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2019.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2019.csv to ./tw_aws_links_list_2019.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2020.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2020.csv to ./tw_aws_links_list_2020.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2021.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2021.csv to ./tw_aws_links_list_2021.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2022.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2022.csv to ./tw_aws_links_list_2022.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ aws s3 cp s3://laelgelctweets/dozent/tw_aws_links_list_2023.csv .\n",
    "download: s3://laelgelctweets/dozent/tw_aws_links_list_2023.csv to ./tw_aws_links_list_2023.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ ls -la\n",
    "total 224\n",
    "drwxrwxr-x 2 ubuntu ubuntu  4096 Nov 14 17:36 .\n",
    "drwxrwxr-x 7 ubuntu ubuntu  4096 Nov 14 17:31 ..\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  1953 Nov 14 17:20 tw_aws.py\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  6579 Nov 14 17:20 tw_aws_links_list_2011.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  4102 Nov 14 17:21 tw_aws_links_list_2012.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  1346 Nov 14 17:20 tw_aws_links_list_2013.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  1353 Nov 14 17:20 tw_aws_links_list_2014.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  1035 Nov 14 17:21 tw_aws_links_list_2015.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  1241 Nov 14 17:20 tw_aws_links_list_2016.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 18107 Nov 14 17:20 tw_aws_links_list_2017.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 16921 Nov 14 17:21 tw_aws_links_list_2018.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 32341 Nov 14 17:20 tw_aws_links_list_2019.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 33187 Nov 14 17:20 tw_aws_links_list_2020.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 30763 Nov 14 17:20 tw_aws_links_list_2021.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu 30092 Nov 14 17:20 tw_aws_links_list_2022.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu  2765 Nov 14 17:21 tw_aws_links_list_2023.csv\n",
    "-rw-rw-r-- 1 ubuntu ubuntu   456 Nov 14 17:20 tw_aws_links_list_test.csv\n",
    "(my_env) ubuntu@ip-172-31-38-165:~/my_env/tw_aws$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a050b-9606-4940-8cbe-8c9bc427d78b",
   "metadata": {},
   "source": [
    "### There are two possibilities of execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5a431-ef24-4d51-b8b8-a7b6f9dfb055",
   "metadata": {},
   "source": [
    "#### On the terminal\n",
    "The terminal should remain open all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477e518-8518-4924-b144-ee09db821284",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-16-95:~/my_env$ python tw_aws.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb2f09-3011-496b-b30d-f736aec1bd60",
   "metadata": {},
   "source": [
    "#### In the background\n",
    "The programme will be executed in the background even if the terminal session is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b2343-1b0c-4850-a17e-b857ec2faf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) ubuntu@ip-172-31-16-95:~/my_env$ nohup python tw_aws.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166f50-97e2-4944-aeb4-fe776a1fc5e7",
   "metadata": {},
   "source": [
    "### Code of 'tw_aws.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e054a81-95da-4a29-8e12-54478ddcb81a",
   "metadata": {},
   "source": [
    "- Tests have shown that:\n",
    "  - 'pySmartDL' does not merge the downloaded parts if it fails to download them. As the downloaded parts are not realiable, the file should be re-downloaded\n",
    "  - If the download of the parts are successful, 'pySmartDL' merges them into one file. During the process, the server must have at least twice the size of the file as free disk space. Therefore, the server should have its disk space sized to twice the size of the largest file to download plus 3 GiB for the operating system\n",
    "\n",
    "- The programme was revised to consider 20 threads of download instead of the default 5 in an attempt to improve the chances of success of the downloads\n",
    "\n",
    "- The programme was revised to implement timestamps and automatic retry in case of exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3a7ad-c882-4344-954a-8d2436f80dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pySmartDL import SmartDL\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Define the name of the CSV file containing the list of links\n",
    "links_list = 'tw_aws_links_list_test.csv'\n",
    "#links_list = 'tw_aws_links_list_2011.csv'\n",
    "#links_list = 'tw_aws_links_list_2012.csv'\n",
    "#links_list = 'tw_aws_links_list_2013.csv'\n",
    "#links_list = 'tw_aws_links_list_2014.csv'\n",
    "#links_list = 'tw_aws_links_list_2015.csv'\n",
    "#links_list = 'tw_aws_links_list_2016.csv'\n",
    "#links_list = 'tw_aws_links_list_2017.csv'\n",
    "#links_list = 'tw_aws_links_list_2018.csv'\n",
    "#links_list = 'tw_aws_links_list_2019.csv'\n",
    "#links_list = 'tw_aws_links_list_2020.csv'\n",
    "#links_list = 'tw_aws_links_list_2021.csv'\n",
    "#links_list = 'tw_aws_links_list_2022.csv'\n",
    "#links_list = 'tw_aws_links_list_2023.csv'\n",
    "\n",
    "# Define the name of the directory where the downloaded files will be stored\n",
    "output_directory = 'tw_aws_data'\n",
    "\n",
    "# Check if the output directory already exists. If it does, remove it and its contents. If it doesn't exist, create it.\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "    print('Old output directory successfully removed.')\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "\n",
    "# Define the name of the S3 bucket\n",
    "bucket = 'laelgelctweets'\n",
    "destination = 's3://{}/'.format(bucket)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(links_list, header=0)\n",
    "\n",
    "# Define the number of threads to use for downloading files\n",
    "threads = 20\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    link = row['Links']\n",
    "    \n",
    "    # Retry the download until it is successful\n",
    "    while True:\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(timestamp, ': Downloading ' + link)\n",
    "            obj = SmartDL(link, output_directory, threads=threads, progress_bar=False)\n",
    "            obj.start()\n",
    "            break  # Break out of the while loop if the download is successful\n",
    "        except Exception as e:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(timestamp, ': Download failed:', e)\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying the download\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(timestamp, ': Retrying ' + link)\n",
    "    \n",
    "    # Get a list of files in the output directory\n",
    "    files_to_copy = sorted(glob.glob(output_directory + '/*'))\n",
    "    \n",
    "    # Copy the downloaded files to the S3 bucket using the aws s3 cp command\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Transferring to ', destination, ' and clearing ', output_directory)\n",
    "    for file in files_to_copy:\n",
    "        subprocess.run(['aws', 's3', 'cp', file, destination], bufsize=0)\n",
    "        subprocess.run(['rm', '-f', file], bufsize=0)\n",
    "    \n",
    "    # Print timestamp after each download\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Download completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
